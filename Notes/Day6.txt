DIFFUSION

right now we have clean Carbon-alpha (CA) coordinates:

x0 = true data (a real backbone)

Diffusion trains like this:
- pick a random time t, make a noisy version: xₜ = √ᾱₜ x₀ + √(1 − ᾱₜ) ε

ε is random Gaussian noise

ᾱₜ controls “how noisy” xₜ is

At small t → xₜ looks almost like x₀
At large t → xₜ looks like pure noise

Step B: denoiser learns to predict the noise

We train a network ε_θ(xₜ, t) to predict ε.

Loss:

L = || ε − ε_θ(xₜ, t) ||²

Once the model learns to predict noise well, you can reverse the process to generate new samples.

